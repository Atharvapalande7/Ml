# -*- coding: utf-8 -*-
"""Lab02_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mZIOAkOu2nYr9QLx6X-LtPQjBVGAas-O
"""

import numpy as np
import matplotlib.pyplot as plt
y=np.array([1.2,1.8,2.6,3.2,3.8])
x=np.array([1,2,3,4,5])
print(x)
print(y)

x_avg=np.average(x)
y_avg=np.average(y)
print(x_avg)
print(y_avg)

C

y_hat=np.average(x**y)
print(y_hat)

a1=(x_hat*y_hat)-x_hat*y_hat
print(a1)

a0=y_hat-a1*x_hat
print(a0)

a1=((y_hat)-(x_avg)*(y_avg))/((x_hat)-(x_avg*x_avg))

a0=(y_avg)-a1*(x_avg)

print(a1)

print(a0)

x=3
y=a0+a1*x
print(y)

x=12
y=a0+a1*x
print(y)

import numpy as np
x=np.array([[[1,1.2],[2,1.8],[3,2.6],[4,3.2]]])
print(x)
print(type(x))
print(x.ndim)

import numpy as np
y=np.array([1.2,1.8,2.6,3.2])
print(y)
print(type(y))
print(y.ndim)

import numpy as np
xtrans=np.transpose(x)
print(xtrans)

import numpy as np
x=np.array([[[1,2,3],[4,5,6],[7,8,9]]])
print(x)
print(type(x))
print(x.ndim)

import numpy as np
x=np.array([1,2,3,4,5])
s=np.sum(x)
print(s)
n=len(x)
r=np.divide(s,n)
print(r)

import numpy as np
x=np.array([[[1,2,3],[4,5,6],[7,8,9]]])
y=np.array([[[3,4,6],[8,7,5],[9,8,10]]])
n=np.add(x,y)
z=np.prod(x)
print(n)
print(z)

import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5])
y = np.array([1.2, 1.8, 2.6, 3.2, 3.8])

plt.plot(x, y)
plt.show()
xsum = np.sum(x)
ysum = np.sum(y)
xavg = np.average(x)
yavg = np.average(y)
xy = x * y
xysum = np.sum(xy)
xyavg = np.average(xy)

num = xyavg
print(f"xysum: {xysum}, xyavg: {xyavg}, num: {num}")

import numpy as np
import matplotlib.pyplot as plt


x_train = np.array([1.0, 2.0, 3.0,5.0,8.0])
y_train = np.array([300.0, 500.0, 800.0,900.0, 500.0])
print(f"x_train = {x_train}")
print(f"y_train = {y_train}")

print(f"x_train.shape: {x_train.shape}")
m = x_train.shape[0]
print(f"Number of training examples is: {m}")

m = len(x_train)
print(f"Number of training examples is: {m}")

i = 0

x_i = x_train[i]
y_i = y_train[i]
print(f"(x^({i}), y^({i})) = ({x_i}, {y_i})")

plt.scatter(x_train,y_train, marker='x', c='r')
plt.title("Housing Prices")
plt.ylabel('Price (in 1000s of dollars)')
plt.xlabel('Size (1000 sqft)')
plt.show()

w = 100
b = 100
print(f"w: {w}")
print(f"b: {b}")

def compute_model_output(x, w, b):
    """
    Computes the prediction of a linear model
    Args:_
      x (ndarray (m,)): Data, m examples
      w,b (scalar)    : model parameters
    Returns
      f_wb (ndarray (m,)): model prediction
    """
    m = x.shape[0]
    f_wb = np.zeros(m)
    for i in range(m):
        f_wb[i] = w * x[i] + b

    return f_wb

tmp_f_wb = compute_model_output(x_train, w, b,)
plt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')
plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')
plt.title("Housing Prices")
plt.ylabel('Price (in 1000s of dollars)')
plt.xlabel('Size (1000 sqft)')
plt.legend()
plt.show()

w = 200
b = 100
x_i = 1.2
cost_1200sqft = w * x_i + b

print(f"${cost_1200sqft:.0f} thousand dollars")

def f(x):
  return x**2
def df(x):
  return 2*x
learning_rate = 0.1
num_iterations = 100
initial_x = 5

x = initial_x
for i in range(num_iterations):
  gradient = df(x)
  x = x - learning_rate * gradient
  print(f"Iteration {i+1}: x = {x}, f(x) = {f(x)}")

print(f"Final value: x = {x}, f(x) = {f(x)}")

import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return x**2
def df(x):
    return 2*x

def cost_function(x, y, w, b):
    n = x.shape[0]
    cost = 0
    for i in range(n):
        j_wb = w * x[i] + b
        diff = j_wb - y[i]
        cost = cost + (diff)**2
    total_cost = 1 / (2 * n) * cost
    return total_cost


def gradient_descent_momentum(x_train, y_train, learning_rate=0.1, momentum=0.9, num_iterations=100, initial_x=5, velocity=0):
    x = initial_x
    x_values = [initial_x]
    f_values = [f(initial_x)]

    for i in range(num_iterations):
        gradient = df(x)
        velocity = momentum * velocity - learning_rate * gradient
        x = x + velocity


        x_values.append(x)
        f_values.append(f(x))

        print(f"Iteration {i+1}: x = {x}, f(x) = {f(x)}")

    print(f"Final value: x = {x}, f(x) = {f(x)}")

    return x_values, f_values


learning_rate = 0.1
momentum = 0.9
num_iterations = 100
initial_x = 5
velocity = 0

x_train = np.array([1.0, 2.0, 3.0, 5.0, 8.0])
y_train = np.array([300.0, 500.0, 800.0, 900.0, 500.0])


x_values, f_values = gradient_descent_momentum(x_train, y_train, learning_rate, momentum, num_iterations, initial_x, velocity)


plt.plot(x_values, f_values, marker='o')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent Optimization with Momentum')
plt.show()


w = 0.1
b = 0.1
cost = cost_function(x_train, y_train, w, b)
print(f"Final cost: {cost}")

