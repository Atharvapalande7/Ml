# -*- coding: utf-8 -*-
"""Lab_1_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wf5V29AiCc9sjido7IYJ7uPJQhBO6k8l

# Quick Primer

This notebook is supposed to a quick primer on what colab/jupyter notebooks are.

Large part of working in a notebook is to work with a python data structure called [pandas](https://pandas.pydata.org/). This notebook is supposed to give you short examples to do simple introductions to this data structure.

A good first step is to watch this [short 10 minute introduction video](https://www.youtube.com/watch?v=_T8LGqJtuGc)

For additional reading see among others:
  + https://towardsdatascience.com/pandas-series-a-lightweight-intro-b7963a0d62a2
  + [Python for Data Analysis book](https://books.google.com/books?id=UWlo-c4WEpAC)
  + [Python Data Science Handbook](https://books.google.com/books?id=6omNDQAAQBAJ)

## Introduction

We are going to be using python as the programming language. What python is, how it works, etc, is outside the scope of this tutorial. There are plenty of much better tutorials that you can find online that explain that.

Python is an interpreted language, with the python interpreter executing one statement at a time. For data science, and for the purposes here we use IPython, which is an enhanced python interpreter. You can run this intepreter many ways, including by using Jupyter or Colab. Jupyter and Colab have a web based interface to run Python code in a notebook, which this here is. A notebook is a type of interactive document for code. It is a mix of text cells and code cells, where the text can be formatted in markdown and the code depends on the interpreter that is attached to it (here we will use python).

### Jupyter

One of the easiest ways to run and install Jupyter is to use a virtual environment in python, which creates a separate Python environment where you can install libraries.

On a Debian Linux system you can run:

```shell
$ sudo apt-get install python3-venv
$ python3 -m venv jupyter_env
$ source jupyter_env/bin/activate
```

This will setup the environment, next up is to install dependencies. These will depend on what you are going to be using this for, etc. For a basic setup
you can install these:

```shell
$ pip install jupyter altair vega-datasets vega pandas jupyter_http_over_ws wheel
$ jupyter serverextension enable --py jupyter_http_over_ws
```

Now you can run jupyter using the command:

```shell
$ jupyter notebook
```

This should automatically open up a web page in your browser window with the Jupyter interface. If not you can navigate to `http://localhost:8888`

To be able to run this notebook in Jupyter you can simply choose "File | Download .ipynb", save the file in the same directory as you started Jupyter and you should see it in the list (might need to refresh the Jupyter browser window)

### Colab

For colab you can simply visit the notebook in [colab's site](https://colab.research.google.com) and click `connect`. The disclaimer here is that it requires you to have an active Google account. All you then have to do is to click the button in the upper right corner that says "`Connect`" and you can start running the code here.

You can also have a mixed environment where you have your own Jupyter kernel
running but using Colab as the frontent, for that you can follow the [instructions here](https://research.google.com/colaboratory/local-runtimes.html) or start the Jupyter notebook using this command:

```shell
$ jupyter notebook \
  --NotebookApp.allow_origin='https://colab.research.google.com' \
  --port=8888 \
  --NotebookApp.port_retries=0
```

Take a note of the authentication URL that appears when running the notebook, it should look something like this:

```
[I 14:39:29.255 NotebookApp] Serving notebooks from local directory: <PATH>
[I 14:39:29.255 NotebookApp] Jupyter Notebook <VERSION> is running at:
[I 14:39:29.255 NotebookApp] http://localhost:8888/?token=<TOKEN>
[I 14:39:29.255 NotebookApp]  or http://127.0.0.1:8888/?token=<TOKEN>
[I 14:39:29.255 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 14:39:29.364 NotebookApp]

    To access the notebook, open this file in a browser:
...
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=<TOKEN>
     or http://127.0.0.1:8888/?token=<TOKEN>
```


Copy the URL That looks like this: “http://localhost:8888/?token=<TOKEN>”. Then click the link for the colab, in the upper right corner click the small triangle and select “Connect to local runtime”

## Start

The first thing we want to do is to install the picatrix library (if we are running this on a cloud instance, can skip if it is already installed)
"""

!pip install picatrix

"""But let's start to import the two libraries that we will be using throghout, pandas and numpy."""

import numpy as np
import pandas as pd

"""You will see this tradition of importing numpy as np and pandas as pd throughout the literature, and therefore we will stick with that here as well.

One simple way to look at pandas is like a database table, where you have column and rows and then you have some operations you can do on these tables/rows.

One thing to keep in mind with an IPython notebook like this is **tab completion**. In the code cells you can press the **tab** key and the namespace will be searched, try it here with the pandas library:
"""

pd.

"""Another useful thing you may want to be aware of is that you can also execute all regular shell commands from a notebook. If a codecell starts with `!` it will be executed in the shell, eg:"""

!ls ~/ && pwd

"""This can come in handy, especially if you are running a local kernel, whether that is from a Jupyter notebook or a colab one attached to a local kernel.

Another option that can come in handy is to use the `?` after a function, to get the docstring from the function (or `??` to get the code):
"""

pd.read_csv?

"""This is referred to as object introspection.

You can also use this to find functions when you mix the use with the wildcard `*` character, such as:
"""

pd.read_*?

"""This will produce a list of all `.read_` functions within the pandas library.

### Magics

Another feature of ipython are [magics](https://ipython.readthedocs.io/en/stable/interactive/magics.html). These are IPython's special commands that are not built into python itself. There are both built-in magics as well as user defined.

Magics are identified by preprending the magic name with either `%` or `%%`, etc. An example of this would be `%run` or %paste` which takes care of running a python file inside a cell, or paste code from your clipboard.

Magics are either line magics (`%`) in which parameters to the magic are all in a single line, or cell magics (`%%`) where the parameters can be defined in using multiple lines.

See more information [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html) or run this command:
"""

# Commented out IPython magic to ensure Python compatibility.
# %magic

"""One useful built-in magic is `%timeit` to time executions of code cells, eg:"""

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# for index in range(0, 10):
#   _ = index * 10

"""This is a meaningless code, but used to demonstrate how you can time the execution of a piece of code using the `%%timeit` magic. In this case it is a cell magic, so all the code in the cell is evaluated.

You can also assign the output of some magics to variables using this:
"""

output = %pwd

output

"""#### Picatrix

The picatrix library or package is geared towards providing analysts with a set of magics and exported python functions that are geared towards security analysis.

To use the picatrix library we need to import it and then initialize, let's do that:
"""

from picatrix import notebook_init

notebook_init.init()

"""To get a list of all magics that are part of the picatrix library, use the `%picatrixmagics` magic:"""

# Commented out IPython magic to ensure Python compatibility.
# %picatrixmagics

"""Each magic is registered in the namespace in three ways:
+ '%magic_name' - line magic
+ '%%magic_name' - cell magic
+ 'magic_name_func()` - a regular python function.

To get further help about each magic, you can run either:

```
%magic_name --help
```

or

```
magic_name_func?
```

Let's look at an example:
"""

# Commented out IPython magic to ensure Python compatibility.
# %picatrixmagics --help

"""### Numpy and Pandas

Let's talk again about numpy and pandas. numpy or Numerical Python is one of the most important fundamental libraries for numerical computing in python. This guide will not go into any details about the inner working of numpy, there are plenty of guides online that provide that if you are interested.

The important part here is that numpy provides a fast C API to work with multidimensional arrays. And it contains mathematical functions to operate on entire arrays of data without requiring use of loops.

#### Numpy Arrays

Let's start to explore a simple numpy array.
"""

arr = np.arange(10000)

"""Let's look at the first 10 entries in the array:"""

arr[:10]

"""And the length"""

print(len(arr))

"""Let's explore the difference in time with a list vs a numpy array"""

arr_list = list(range(10000))
print(len(arr_list))

"""Let's look at the time when muliplying an array vs a list with a number:"""

# Commented out IPython magic to ensure Python compatibility.
# %time a = [x * 10 for x in arr_list]

# Commented out IPython magic to ensure Python compatibility.
# %time a = arr * 10

"""Numpy arrays can also be multidimensional:"""

arr = np.random.randn(4, 4)

arr

"""This can also be multiplied, eg:"""

arr * 3

arr + arr

"""You can also select a single entry of the array, eg if we want the element in the first row, third column (remember counting starts from zero):"""

arr[0, 2]

"""Or we can choose a subset, using slicing.

Let's choose the first two rows:
"""

arr[0:2]

"""or the middle elements of the first two rows:"""

arr[0:2, 1:3]

"""You can also index based on strings if the array is an array of strings, eg:

"""

arr = np.array(['time', 'picatrix', 'sketch', 'magic', 'wizard'])

arr

"""To get a boolean array that can be used for filtering we construct a boolean query:"""

arr == 'time'

arr[arr == 'time']

"""There are many more important properties of numpy arrays that we will not have time to cover in this very short introduction.

### Pandas Series

Let's move to talking about python pandas. The biggest difference between pandas and numpy is that pandas are designed to work with tabular data, think more of a spreadsheet or a database.

Pandas defines two majore data structures, *Series* and *DataFrame*. A *Series* is a one-dimensional array-like object containing a sequence of values. An example can be:
"""

pd.Series(['a', 'b', 'c', 'd', 'e'])

"""Series can also have indexes or labels attached to each value (where they are then almost similar to a dict)"""

ser = pd.Series(['a', 'b', 'c', 'd', 'e'], index=['foo', 'bar', 'more', 'note', 'extra'])

ser

"""Now you can access each object using the dot notation or brackets"""

ser.foo

ser['foo']

"""We can also convert a series object into a dict, or create a Series object from a dict:"""

ser.to_dict()

ser = pd.Series({
    'stuff': 134,
    'more': 11,
    'notes': 'extra stuff'
})

ser

ser.notes

"""There are many built-in functions to [work with series](https://pandas.pydata.org/pandas-docs/stable/reference/series.html) that we will not have time to cover in this tutorial. But for the purpose of analyzing text data take special care of `str.contains` and `str.extract`, eg:"""

ser.str.contains('stuff')

ser.str.extract(r' (s[^ $]+)')

"""#### Pandas DataFrame

A DataFrame, which is the object you may work with the most is a rectangular table of data and contains an ordered collection of columns. You can think off it as a dict of Series, all sharing the same index.
"""

lines = [
    {'Important': True, 'Value': 1345, 'Notes': 'Stuff IS Stuff'},
    {'Important': True, 'Value': 23, 'Notes': 'This does not contain any word...'},
    {'Important': True, 'Value': 523, 'Notes': 'We have a lot of text in here, including stuff'},
    {'Important': False, 'Value': 100, 'Notes': 'Here is a word that sounds like stuff but is in fact soooo much longer'},
]

df = pd.DataFrame(lines)

df

"""We can start by looking at the shape of the dataframe:"""

df.shape

"""This tells us that it contains four rows and each row has three columns. Let's look at the first two rows:"""

df.head(2)

"""Or the last 2:"""

df.tail(2)

"""We can also just look at the value of a single column:"""

df['Value']

"""Which will give us back a Series object, that we can then use all Series operations on."""

df['Notes'].str.contains('stuff')

"""We can then also use this filtering to filter out the rows in the dataframe. So to get only the rows that contain the word `stuff` we can do:"""

df[df['Notes'].str.contains('stuff')]

"""We can also filter out all the non important values:"""

df[df['Important']]

"""We can also assign values here:"""

df['NewValue'] = 5452

"""This will apply to the entire dataframe:"""

df

"""You can also create values that contain parts of other columns"""

df['message'] = df['Notes'] + ' --> ' + df['Important'].astype(str) + ' [' + df['Value'].astype(str) + ']'

df

"""We can also extract values from one string and assign it to another."""

df['stuff'] = df['Notes'].str.extract(r'\b([sS][^ $]+)')

df

"""#### Counting And Unique Values

Another very useful property is the ability to summarize the data:
"""

df['Important'].value_counts()

"""Or finding all unique values of a column:"""

df['Important'].unique()

"""There are also two built-in functions that provide an overview of the data:"""

df.info()

df.describe()

"""#### Slicing a DataFrame

A dataframe can be sliced by using boolean filters, eg:
"""

df[df['Important']]

"""These can be combined to give more granular results:"""

df[(df['Important']) & (df['Notes'].str.contains('stuff', case=False))]

"""The slices can be saved to further filter as well:"""

df_slice = df[(df['Important']) & (df['Notes'].str.contains('stuff', case=False))]

df_slice[df_slice.Value > 1000]

"""#### Direct Selection

There are 2 main ways to retrieve subsets of a dataframe:

+ `.iloc[]`
+ `.loc[]`

`.loc` is **label** based.
`.iloc` is **position** based.

Both of these can be used in 5 different ways:

1. Single row
2. List of rows
3. Slice
4. Boolean mask
5. A function (providing the dataframe as input), which returns any of the above 4

To get the first row of the dataframe use `iloc`

"""

df.iloc[0]

"""Or the first two rows:"""

df.iloc[0:2]

"""`iloc` uses the integer position within the dataframe whereas `loc` uses labels as previously stated. In this case the label is also an integer."""

df.loc[2]

"""We can also pick a column"""

df.loc[2:3, 'Value']

"""We can also change the labels here:"""

df['NewIndex'] = pd.Series(['A', 'B', 'C', 'D'])
df.set_index('NewIndex', inplace=True)

df

"""Now we can use the new index"""

df.loc['B']

"""Or get a slice:"""

df.loc['B':'D']

"""#### Sorting

We can also sort the dataframe:
"""

df.sort_values('Value')

"""Or in descending order:"""

df.sort_values('Value', ascending=False)

"""#### Ranks

Ranking replaces each valid value in a dataframe with its ordinal if the dataframe were sorted by that column (ties are given the mean of the ranks).
"""

df.rank()

"""Or rank by columns"""

df.rank(axis='columns')

"""If the values are numeric you can also summarize the values using functions like `sum`:"""

df.Value.sum()

df.Value.mean()

df.Value.cumsum()

"""### Reading in Data

One of the most important parts of using pandas is to read the data in. If you don't have data then it's hard to work with it.

Pandas provide a whole heap of methods of getting data, from connecting to SQL databases to spreadsheets to CSVs. This tutorial will only cover the most basics.
"""

pd.read_*?

"""This will give you some overview of what functions are available. If you want to know more about a certain function type in `pd.read_excel?`

Let's look at the most basic function, which is to read in a CSV file. Since we don't have a file, we will just generate a very basic one and then read it in.
"""

import csv

with open('/tmp/foobar.csv', 'w') as fw:
  writer = csv.writer(fw)
  writer.writerow(['First', 'Second', 'Third'])
  writer.writerow([1, 2, 4])
  writer.writerow([5, 3, 2])
  writer.writerow([1, 3, 0])

df = pd.read_csv('/tmp/foobar.csv')

df

"""If the CSV file is really large, the data can be read in chunks:"""

for chunk in pd.read_csv('/tmp/foobar.csv', chunksize=2):
  print(chunk.shape)

"""There are many more nuances in importing data that will not be covered in this basic tutorial.

## Filling in Missing Data

**TODO: Fill this out**
"""



"""## Manipulating the Data

**TODO: Fill this out, add how to change values, use `.apply` and other functions to change values or add new columns**
"""

